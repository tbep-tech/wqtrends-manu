---
title: "Multi-scale trend analysis of water quality using error propagation of generalized additive models"
output: 
  bookdown::word_document2:
    reference_docx: my_styles.docx
    number_sections: false
always_allow_html: true
bibliography: refs.bib
author: "Marcus W. Beck (mbeck@tbep.org), Perry de Valpine (pdevalpine@berkeley.edu), Rebecca Murphy (rmurphy@chesapeakebay.net), Ian Wren (ianw@sfei.org), Ariella Chelsky (ariellac@sfei.org), Melissa Foley (melissaf@sfei.org), David B. Senn (davids@sfei.org)"
urlcolor: blue
csl: ecology.csl
link-citations: true
---

```{r setup, echo = F, warning = F, message = F, results = 'hide'}
# figure path, chunk options
knitr::opts_chunk$set(fig.path = 'figs/', warning = F, message = F, echo = F, cache = F, dev.args = list(family = 'serif'), dpi = 300, warning = F,
  fig.process = function(x) {
  x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
  if (file.rename(x, x2)) x2 else x
  })

# libraries
library(Jabbrev)
library(tidyverse)
library(wqtrends)
library(flextable)
library(officer)
library(ggmap)
library(sf)
library(ggsn)
library(USAboundaries)
library(rworldmap)
library(hrbrthemes)
library(patchwork)
library(lubridate)
library(colorspace)

data(locs)
data(modprf)
data(seastrnd)
data(seastrnd2)
data(cmptrnd)
data(modstr)
data(modslog_chl18)
data(modslog_chl24)
data(modslog_chl27)
data(modslog_chl32)
data(modslog_chl34)

source('R/funcs.R')

locs <- locs %>% 
  rename(station = Station)

# # extract bib entries from online
# bib_scrp('manu_draft.Rmd', 'refs.bib')
```

`r paste('Last manuscript build', Sys.time())`

```{r echo = F, cache = F, eval = F}
spelling::spell_check_files('manu_draft.Rmd')
```

# Abstract

Accurate and flexible trend assessment methods are valuable tools for describing historical changes in environmental monitoring datasets.  A key requirement is complete propagation of uncertainty through the analysis.  However, this is difficult when there are mismatches between time scales of monitoring data and trends of interest. Here, we propose a novel application of generalized additive models (GAMs) to model seasonal and multi-decadal changes in a long-term monitoring dataset of chlorophyll-a concentrations in the San Francisco Estuary.  GAMs can model a response variable as the additive sum of one or more smoothing splines fit to predictor variables.  These models have shown promise in water quality trend analysis to separate long-term (i.e., annual or decadal) trends from seasonal variation.  Our proposed methods estimate seasonal averages in a response variable with GAMs and then use the uncertainty measures with mixed-effects meta-analysis regression to quantify inter-annual trends that account for full propagation of error across methods.  We first demonstrate that nearly identical descriptions of temporal changes can be obtained using different smoothing splines for annual or seasonal components of the time series.  We then extract seasonal averages and their standard errors for an *a priori* time period within each year from the GAM results.  Finally, we demonstrate how across-year trends in seasonal averages can be modeled with mixed-effects meta-analysis regression that propagates uncertainties from the GAM fits to the across-year analysis. Overall, this approach leverages GAMs to smooth data with missing observations or varying sample effort across years to estimate seasonal averages and meta-analysis to estimate trends across years.  Methods are provided in the *wqtrends* R package.    

# Introduction

Accurate quantification of trends must consider variation at different temporal scales regardless of the question of interest, such that ignoring variation at one scale could lead to incorrect conclusions about variation at another scale. Many environmental monitoring programs collect temporally resolved but irregular time-series data to quantify trends for regulatory, management, or research purposes.  The mismatch between the scales of monitoring versus analysis questions or management goals can present statistical challenges [@Urquhart98;@Cumming06;@Forbes18]. At short temporal scales typically less than a year, environmental systems exhibit variability caused by multiple factors (e.g., weather events, management, or seasonal changes).  Such fluctuations may not be related to inter-annual trends or may not be well-suited to multi-scale smoothing methods.  In this paper, we develop methods to estimate across-year trends of within-year features, such as seasonal averages, while accounting for uncertainties across analysis steps.  

Existing methods that begin to address our objectives in water quality trend analysis can be generalized into four basic approaches: seasonal Kendall tests, seasonal trend decomposition (STL), weighted regression on time, discharge, and season (WRTDS), and generalized additive models (GAMs).  Seasonal Kendall tests or related non-parametric approaches have been used for decades in water quality trend assessments to identify monotonic changes over several years while accounting for the predictable patterns among seasons [@Hirsch82;@Helsel20]. @Wan17 showed that non-parametric approaches have been the most commonly used methods in long-term water quality trend analysis despite critical limitations.  For descriptive decomposition of long-term monitoring data, they assume seasonal patterns within years do not change, require regularly spaced or balanced data, only use time as a descriptor for trends, and do not estimate a model that could be useful for other purposes.  Thus, while these non-parametric approaches have some degree of robustness to assess magnitude and direction of trends, they apply only to narrow goals.

The seasonal trend decomposition using loess (STL) decomposes a time series into additive components of a long-term trend, a seasonal pattern, and residuals [@Cleveland90;@Cloern10;@Stow15].  While useful and widely applied, this method also has important limitations.  STL decomposition does not incorporate explanatory variables besides time, it is defined more as an algorithm of statistical steps than as a coherent statistical model [e.g., @Wan17], and it does not usually estimate standard errors to allow hypothesis testing [but see @Hafen10].  STL methods may also over-simplify trends into stationary components that do not change over time, e.g., a seasonal estimate that is constant across years.  This limitation presents challenges when addressing questions relevant to long-term water quality data, such as timing of seasonal peaks that can suggest system response to changing  environmental conditions [@Cloern10;@Navarro12].

The weighted regression on time, discharge, and season (WRTDS) method addresses the problem of inflexibility in STL by using a more general local regression scheme [@Hirsch10;@Beck15;@Beck18b].  Designed for evaluating water quality in rivers where separating the effect of discharge on constituent concentration is important, WRTDS estimates a moving window regression model with components that allow parameters to vary smoothly in relation to both time and discharge.  This yields parameters that are specific to season, year, and flow regime.  The WRTDS approach is conceptually similar to local kernel smoothing methods, with specific application to explanatory variables relevant for water quality constituents (i.e., season, year, and discharge). Standard error estimates of predictions from WRTDS are available through a block bootstrap approach applied to the model results [@Hirsch15]. Although a useful addition to the original method [@Hirsch10], the approach requires extensive resampling as a *post-hoc* application to a previously fitted model.  Alternative methods that include standard error estimates simultaneously with model output may be preferred for intensive or more iterative applications

Generalized additive models (GAMs) are central to this paper and form the basis of our fourth and last method to separate fluctuations on different time scales.  GAMs combine one or more smoothing splines to model patterns in data and may be seen as generalizing the concepts behind STL and WRTDS [@He06;@Morton08;@Pearce11;@Haraguchi15;@Murphy19]. The basis functions used to formulate GAMs can be customized based on expected patterns in the data. Examples include cyclic splines, which can be used to model seasonal patterns, and low-dimensional interactions.  GAMs have added flexibility because they can include both parametric (e.g., linear or quadratic) components and non-parametric (spline) components.  Multiple approaches have been developed to determine the optimal degree of smoothness.  These approaches are based on optimization of out-of-sample prediction error, which addresses a key concern around methods like WRTDS that do not have analogs for choosing optimal degrees of smoothing.  GAMs can also produce comparable results similar to those provided by WRTDS [@Beck17] and have readily obtainable uncertainty estimates that do not require *post-hoc* resampling of model results. Further, GAMs have natural frequentist and Bayesian interpretations, are naturally extensible to include random effects (i.e., generalized additive mixed models or GAMMs), and have computationally efficient implementations [@Wood17].

GAMs have been applied previously to evaluate trends in water quality time-series from long-term monitoring programs [@Haraguchi15;@Murphy19]. For example, @Murphy19 used GAMs to decompose water quality time series from Chesapeake Bay into long-term and seasonal trends [@Murphy19] and test trend hypotheses between two points in time.  Other studies of environmental time-series with GAMs have addressed the use of transformed response data [@Yang20], serial correlation in high resolution data [@Morton08;@Yang20], and quantifying time lags in relationships between response and predictor variables [@Lefcheck17]. The study herein generalizes the approach to analyzing trends of seasonal spline features, describes the relationships among alternative spline formulations when spline flexibility is allowed to vary [@Wood03;@Wood17] rather than being constrained *a priori* for different time scales, and prioritizes full incorporation of uncertainty.  

Our motivating problem has several characteristics that are only partially addressed by previous methods and can further build on  GAMs as a starting point. Our general goal is to understand interannual changes in seasonally averaged water quality metrics, such as chlorophyll.  However, the seasonal average within each year must be robust to inconsistent sampling times and intervals, and any trend analysis must consider the uncertainties in seasonal averages.  The critical need is the ability to obtain an accurate estimate of uncertainty (e.g., a standard error) of seasonal averages, even with irregular sampling and serial correlation, which is common in time series data.  This paper develops the use of GAMs with mixed-effects meta-analysis [@Gasparrini12;@Sera19] to address multi-scale trend analysis questions for which seasonal Kendall tests and the more complex STL and WRTDS methods are not well-suited.

We describe and demonstrate the proposed methods by analyzing water quality monitoring data from the southern portion of the San Francisco Estuary, California, USA.  Approximately twice-monthly monitoring has been conducted for several decades at fixed locations (stations) on the longitudinal axis of the Bay.  Analysis of these data is complicated by irregularities in timing and consistency of data collection which can generate artifacts affecting simple seasonal averages of the data.  We were interested in questions such as: Are there significant trends in spring mean chlorophyll at multi-year time-scales?  At what across-year scale do summer-fall mean chlorophyll levels change?  Are there significant across-year trends in the spring phytoplankton bloom or baseline chlorophyll concentrations during periods of low productivity between blooms?  We provide examples illustrating how these questions can be addressed using GAMs to estimate seasonal trends and evaluated between years using meta-analysis methods. This approach is new to environmental trend-detection problems and is provided in the *wqtrends* R package developed by the authors [@Beck21, available at https://tbep-tech.github.io/wqtrends].

# Methods

## Study area and data sources

The San Francisco Estuary (SFE) is the largest estuary on the Pacific Coast of North America.  Its watershed covers 200 thousand km$^2$ in the US state of California. Major freshwater inputs enter the system through the Sacramento-San Joaquin Delta complex upstream of Suisun Bay, where the combined inflow from both rivers is approximately 28 km$^3$ per year.  Salinity ranges from 0 to 15 ppt in the northern subembayments and from 5 to 35 ppt in southern subembayments closer to the Pacific Ocean, depending on the tidal cycle, effluent discharge from wastewater treatment plants, and stormwater runoff.  An estimated 73.8 metric tons dy$^{-1}$ of inorganic nitrogen are discharged into the Bay, primarily from wastewater [@Novick14]. Agricultural runoff from the upper watershed contributes 30 metric tons dy$^{-1}$ of nitrogen to the SFE via the Delta.

Nitrogen and phosphorus levels in the SFE usually exceed concentrations that cause eutrophication in other estuaries.  However, the SFE has demonstrated resistance to eutrophication, which has been attributed to high concentrations of suspended sediments that reduce light penetration in the water column, low residence time caused by vigorous river flushing, and removal of primary producers by abundant suspension feeding bivalves [@Cole84;@Alpine88;@Jassby08;@Kimmerer14;@Lehman17].  The Regional Water Quality Control Board has showed renewed interest in understanding the potential for nutrient loading to negatively affect water quality for more southern areas of the SFE, where harmful algal blooms, elevated summer-fall chlorophyll concentrations, and low dissolved oxygen concentrations began around 1999 (Figure \@ref(fig:obsdat)) [@Cloern20]. Although changes in the data are visually apparent, statistical analyses to quantify these changes have been insufficient particularly with respect to seasonal differences between years.

We evaluated near-surface chlorophyll (chl-a) data measured biweekly to monthly from 1990 to 2019 along the longitudinal axis of the SFE extending from Central Bay (stations 18-23), South Bay (stations 24-32), and Lower South Bay (stations 34-36) (Table \@ref(tab:sumtab), Figure \@ref(fig:sitemap)). Monitoring data were obtained from the SFE Research Program of the US Geological Survey [@Cloern16;@Schraga20]. Sampling frequency varied somewhat over time and by station.  Every observation was included directly in the statistical models without spatial or temporal binning or averaging. Log$_{10}$-transformed chl-a was used for all analyses to meet assumptions of normally-distributed residuals. Methods for back-transformation of model results are provided in the supplement.

## GAM application

We implemented our analysis in three stages.  First, we use a GAM to estimate a smooth temporal pattern in the raw data along with its uncertainty.  Second, we calculate a feature of interest from the estimated GAM, along with its propagated uncertainty. For this example, the seasonal averages were extracted, whereas other features could be the timing or magnitude of a seasonal peak, but those are not developed here.  Third, we use a mixed effects meta-analysis to estimate trends and test hypotheses about the change in seasonal averages across years.  While meta-analysis methods arose from analyses of results from multiple studies, their distinguishing characteristic is propagation of uncertainty [@Gasparrini12;@Sera19].  Meta-analysis uses response data that includes standard errors (uncertainties) as needed to address our questions.

### First-stage analysis: GAM estimation

We considered four different GAMs to smooth the raw data across time. Although they can achieve similar fits, they do so by partitioning variation in the time-series differently (Table \@ref(tab:modsumtab)). We discuss all four to clarify their relationships and interpretations.  Models are shown in the notation of the `mgcv` R package as formulas for the `gam` function [@Wood17;@RCT20]. 

The simplest GAM for this purpose is expressed as:

Model S: `y ~ s(cont_year, k = num_knots_Y)`

where `y` is the time-series of interest, such as chl-a, `cont_year` is "continuous year", a continuous numerical date (e.g., July 1st 2019 would be 2019.5), `y ~ s(...)` indicates that `y` will be explained by a smoothing spline (in this case as a function of `cont_year`), and `num_knots_Y` is the number of knot or "connections" along the spline that influence curvature.

Smoothing was determined using generalized cross-validation (GCV, as implemented in `mgcv`), which approximately minimizes out-of-sample prediction error.  GCV works by penalizing the net curvature of a spline [@Wood04].  To allow GCV (or other alternatives) to work as intended, the number of knots that determine the maximum degrees of freedom chosen by the analyst must be sufficiently large so that the curvature penalty, rather than the number of knots, determines smoothness.  Results should not be sensitive to the number of knots; if they are, the number of knots should be increased.  In the examples below, we chose the number of knots, `num_knots_Y`, as 12 times the number of years in the time series, i.e., one knot per month. If the data were too sparse to fit 12 knots per year, the number of knots was reduced by one knot per year until the model could be estimated (i.e., 12 * years, 11 * years, etc.).   

The next three spline formulations (Model SY, SYD, and SYDI) provide progressively increasing complexity in how spline terms compose a model to smooth the raw data.  Model SY describes the time series using a linear trend plus a spline for `cont_year`:

Model SY: `y ~ cont_year + s(cont_year, k = num_knots_Y)`

This model is mathematically equivalent to model S (Table \@ref(tab:modstrtab)).  The spline for `cont_year` includes an unpenalized linear trend, so a trend will be estimated in model S.  When `cont_year` is included explicitly as a linear term in model SY, `mgcv` adjusts the basis functions for the spline to exclude the linear term, thereby not over-parameterizing the model. Whereas an estimated linear trend in `cont_year` and its uncertainty can be extracted from the fitted spline in model S, model SY provides this trend directly, giving the equivalent result.  Further, package `mgcv` can penalize linear trends in splines to provide a method for variable selection (option `select = TRUE`), such as when numerous splines are included in the model formulation for variables that may or may not be important.  For our approach, this option is not used and all models specify `select = FALSE`.  Details in the supplement explain this justification. 

Model SYD adds an average within-year cyclic pattern as a separate spline:

Model SYD: `y ~ cont_year + s(cont_year, k = num_knots_Y) + s(doy, bs = 'cc', k = num_knots_D)`

where `doy` is "day-of-year" (i.e., Julian date, a count starting January 1 for each year), `bs = 'cc'` indicates that the spline will be cyclic (constrained to start and end at the same value), and `num_knots_D` is the number of knots for the `doy` spline.  While model SYD is not mathematically equivalent to models S and SY, it should produce nearly identical results.  The `doy` spline in model SYD gives the average seasonal pattern and changes the interpretation of the `cont_year` spline to represent smoothed deviations from the average within-year pattern.

Models S, SY, and SYD can all potentially extract a similar signal from the raw data (Table \@ref(tab:modstrtab)).  What differs between the models is the allocation of penalties for curvature used to determine smoothness for each spline.  In model SYD, there are separate penalties for the two splines, as compared to S and SY that include penalties only for the `cont_year` spline. This is important because variation in the response variable can be differently attributed to each spline depending on the model, even while the sum of components for each model produces similar results between models.  Our goal is to extract seasonal averages from the fitted time series, which is not sensitive to different allocation of penalties among the splines in each model.  

If the fits were to differ substantially between model SYD and models S or SY, an interpretation could be difficult because the penalties for smoothing splines based on curvature are heuristic [@Wood17]. For example, if a lower AIC is achieved in one model compared to another, assuming both use sufficient knots, this may just reflect the outcome of alternative penalization heuristics implied by the different formulations and does not imply one model fit is better. In the examples here, model SYD achieves nearly identical fits to model S or SY, where the latter by definition also achieve identical fits.

Model SYD has the appealing feature that, if some parts of some years have limited data, model SYD will impute an average seasonal pattern with the `doy` spline, thereby considering data from the same period in other years in the prediction of the period with missing data. However, an interpretation of these imputations may be challenging.  For example, the spring chl-a peak is a notable feature every year in the SFE.  If the peak occurs at the same time every year but the magnitude varies, then the average within-year pattern can be interpreted as the average magnitude. However, if the magnitude is the same but the timing varies across years, then the magnitude of the average peak cannot be similarly interpreted and instead underestimates the magnitude that usually occurs.  Moreover, the width or duration of the peak will be longer than typically occurs in a given year.

Finally, the raw data can be smoothed using a bivariate spline representing an interaction between `cont_year` and `doy`. This can be expressed as:

Model SYDI: `y ~ cont_year + s(cont_year, k = num_knots_Y) + s(doy, bs = "cc", k = num_knots_D) + ti(cont_year, doy, bs = c("tp", "cc"), k = c(num_knots_Y_ti, num_knots_D_ti))`

where `ti()` specifies a tensor-product spline for a surface that varies smoothly as a function of both `cont_year` and `doy`.  The number of knots is the product of `num_knots_Y_ti` in the `cont_year` axis and `num_knots_D_ti` in the `doy` axis.  In SYDI, the need for sufficient knots can be satisfied either by sufficiently large values for `num_knots_Y_ti` and `num_knots_D_ti` or a sufficiently large value for knots in `num_knots_Y` and `num_knots_D`, but not both given limits on the model degrees of freedom.

Following the rationale above, the relationship of model SYDI to model S is similar to that of model SYD to model S.  Model SYDI differs formulaically from model S to a greater extent than model SYD, but all of the splines use the same inputs to smooth the same data.  The univariate splines in `cont_year` and `doy` will likely not capture as much variation in model SYDI compared to model S given the fewer knots that are available to the former.  The `ti` term represents an interaction by allowing the pattern in `cont_year` to vary by `doy` and vice-versa.  The interaction term in model SYDI provides an appearance that this model is fundamentally different from those provided by the other models.  However, models S, SY, and SYD all allow within-year fluctuations to vary across years by allowing a spline to be fit through the entire time-series.  Although model SYDI is the only model that includes an explicit interaction term, all of the models support the interaction conceptually. By providing this term with sufficient knots, the raw data can be fully smoothed with model SYDI to a similar degree as for the other models.  However, a very large number of knots in both the `cont_year` spline *and* in both dimensions of the interaction spline is impossible to achieve. The distinct aspect of model SYDI is the anticipation that within-year fluctuations will vary smoothly from year to year and that feature could be accounted for with this term, but for the SFE data and chl-a dynamics in many estuaries, bloom size typically varies between years. Thus, the conceptual motivation for model SYDI and its practical application are not supported for this analysis.

@Murphy19 used spline formulations for Chesapeake Bay water quality related to those proposed here, but for different goals and with different handling of smoothness.  They evaluated a "`gam0`" with only `s(doy)` and linear `cont_year` terms, a "`gam1`" like our SYD, and a "`gam2`" like our SYDI.  In application, only "`gam2`" was used, including the addition of splines as functions of hydrologic variables to account for finer-scale variation. Murphy et al. allowed a maximum number of knots in the `s(cont_year)` term of 2/3 times the number of years and do not explicitly consider the number of knots in the interaction spline, following an *ad hoc* allocation of variation in the data to different components based on previous interpretations of water quality dynamics in the system. Constraining splines with insufficient knots could inflate Type I error rates for temporal changes and we seek to lower this risk by increasing the upper limit for the knots for the `s(cont_year)` term.  Finally, @Murphy19 present large AIC differences between their spline formulations.  We instead emphasize that, given sufficient knots, the models represent alternative formulations of conceptually similar explanations for the data and yield similar fits (Table \@ref(tab:modstrtab)), resulting in near ties for AIC between models. 

We visually compare chl-a estimates from models SY, SYD, and SYDI to emphasize that similar fits can be achieved by all of the presented models (Figure \@ref(fig:modsumfig), S is identical to SY and is not shown).  Models SY, SYD, and SYDI were fit to chl-a data from station 34 using large k values for the arguments `num_knots_y`, `num_knots_D`, `num_knots_Y_ti`, and `num_knots_D_ti` for each model.  Predictions by day of year from each model are visually similar (Figure \@ref(fig:modsumfig)a) and closely follow the 1:1 line (Figure \@ref(fig:modsumfig)b).  However, when contrasting the estimates using only the continuous year smoother (`s(cont_year)`), the fits differ substantially because of how each model allocates variation to the splines.  These results are also reflected in differences in the effective degrees of freedom among the additive components of each model (Table \@ref(tab:modstrtab)). Accordingly, even though the models differ by which structural component describes variation in the chl-a time series, they provide similar predictions. 

For all results, model S was used with enough knots in `num_knots_y` to evaluate chl-a trends across the monitoring stations in the SFE.  This model was chosen because of the relatively faster processing time to fit the model, while providing nearly identical explanatory power as compared to the other models (Table \@ref(tab:modstrtab)). 

### Second-stage analysis: Uncertainty propagation from estimated GAMs to seasonal features

\newcommand{\hm}{\hat{\mu}}
\newcommand{\hmt}{\hat{\mu}_t}
\newcommand{\mt}{\mu_t}
\newcommand{\hmr}{\hat{\mu}_r}
\newcommand{\hshmt}{\hat{\sigma}_{\hat{\mu}, t}}
\newcommand{\hshmr}{\hat{\sigma}_{\hat{\mu}, r}}
\newcommand{\hshm}{\hat{\sigma}_{\hat{\mu}}}
\newcommand{\hsshm}{\hat{\sigma}^2_{\hat{\mu}}}
\newcommand{\hsshmt}{\hat{\sigma}^2_{\hat{\mu}, t}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\hby}{\hat{\mathbf{y}}}
\newcommand{\hbb}{\hat{\boldsymbol{\beta}}}
\newcommand{\hShb}{\hat{\Sigma}_{\hat{\beta}}}
\newcommand{\hShy}{\hat{\Sigma}_{\hat{\mathbf{y}}}}
\newcommand{\sbs}{\sigma_b^2}

In the second-stage analysis, we estimated a seasonal average, such as the mean spring chl-a concentrations and the uncertainty in each year.  We define $\mu_t$ as the seasonal average in year $t$, $\hmt$ as an estimate of $\mu_t$, and $\hshmt$ as the estimated standard error of $\hmt$. The season includes $n$ days.  For simplicity, the following text omits subscript $t$.

Point estimates of response values for the fitted GAM take the form $\hby = \bX \hbb$, where $\hbb$ is the vector of parameter estimates and $\mathbf{X}$ is a model matrix of explanatory variables, including spline basis function values.  Vector $\hbb$ includes both fixed effect parameters and spline parameters, and $\bX$ contains columns corresponding to each.  For example, using model SY, if a point estimate for chl-a is needed for a single day, given as `dec_year` = $r$, then $\bX$ would have a row with $1$ in the first column (for the intercept parameter), $r$ (for the linear time trend) in the second column, and an evaluation of each spline basis function at $r$ in the remaining columns. The number of spline basis functions is related to the number of knots. Note that $r$ can be any time, not necessarily the time of an observation.

To obtain a vector, $\hby$, of fitted point estimates for every day in a season, $\bX$ would have one row for each day.  Here, the seasonal averages used in our examples were calculated at the resolution of days.  The estimated spline yields both $\hbb$ and $\hShb$, an estimate of the covariance matrix of the sampling distribution of $\hbb$.  The scalar standard errors of $\hbb$ are the square roots of the diagonal elements of $\hShb$, whereas the off-diagonal elements are the correlations among the elements of $\hbb$.  Since parameter estimates are correlated, the covariance of $\hby$ is $\hShy = \bX \hShb \bX^T$.

The estimated seasonal average was calculated from the vector of daily values for each of the $n$ days in the season of interest with $\hm = A^T \hby$, where $A^T$ is a row vector with all values equal to $1/n$.  The variance of $\hm$ is $\hsshm = A^T \hShy A$ and standard error is $\hshm$.  Each of these estimates are from on approximate multivariate normality of the sampling distribution of $\hbb$.

### Third-stage analysis: Trend analysis of seasonal features with uncertainties

In stage three of the analysis, we used a meta-analysis method to evaluate linear trends across years of seasonal-average water quality, characterized by the within-year means ($\hmt$) and their standard errors ($\hshmt$) that we estimated in stage two of the analysis. This analysis provided a direct answer to the question: Is there a significant linear trend across a group of years in a seasonal average, where the time-scale of the seasonal average and duration of the across-year comparison is chosen by the investigator?  For example, is there a trend in the spring chl-a average from 1990 to 2000? This question can also be posed in a moving-window manner across a time-series (e.g., spring average trend from 1990-2000, 1991-2001, etc.). For all analyses, the response data of interest are $\hmt$, $t = 1, \ldots, N$, with their associated standard errors, $\hshmt$.  $N$ is the number of years of the study. 

A mixed effects meta-analysis model can estimate linear trends when each observation has an associated standard error, which is the case with our estimates $\hmt$ and $\hshmt$.  Differences in standard errors, which may result from different monitoring effort between years, are explicitly considered in the analysis.  The model can be expressed using notation similar to @Sera19:

\begin{equation}
\hmt = \beta_0 + \beta_t t + b_t + \epsilon_t
(\#eq:mixmet)
\end{equation}

where $\beta_0$ is the intercept, $t$ is the year, $\beta_t$ is the slope, $b_t$ is the random effect for year $t$, and $\epsilon_t$ is the residual for year $t$. Accordingly, the seasonal average for year $t$ is $\mu_t = \beta_0 + \beta_t t + b_t$.  The "residual", $\epsilon_t$, represents estimation error in $\hmt$, namely $\hmt - \mt$.  The residuals are assumed to be independent and normally distributed with mean 0 and variance $\hsshmt$, where the latter is estimated from the calculations above.  The random effect, $b_t$, is the difference between $\mt$ and $\beta_0 + \beta_t t$ and is considered the "residual" in the sense of unexplained variation not due to the estimation error.  The random effect follows a normal distribution with mean 0 and variance, $\sbs$, to be estimated.

We estimated the model (equation \@ref(eq:mixmet)) using the *mixmeta* package in R [@Sera19]. Results from *mixmeta* have a similar interpretation as those from regression analysis, but parameter estimates and their standard errors incorporate the known standard errors of the response values.  Following meta-analysis theory, $mixmeta$ evaluates this model as a linear mixed effects model with fixed and estimated ($\sbs$) variance components.  The default estimation method for *mixmeta*, restricted maximum likelihood (REML), was used. The meta-analysis models were applied to a chosen sequence or "window" of years for estimating the linear trend.  This approach evaluated whether there was a significant linear trend and provided an estimated rate of change (log$_{10}$ slope) in chl-a over a chosen series of years. 

### Trend comparisons

The above methods were applied to each station by evaluating changes in seasonal averages from January to June and July to December for approximate ten year moving windows from 1991 to 2019. The choice of water-year seasons are relevant to phytoplankton bloom phenology in the SFE [@Cloern20]. The moving-window approach applied the meta-analysis to each decadal window (e.g., 1991-2001, 1992-2002, etc.), allowing changes in slope and its significance to be interpreted as the window is shifted one year at a time.  We interpret the slope as representative for the central year for each block, but a predictive trend for the final year of the window could also be interpreted. For some results, we focus on the windows 1990-2000, 200-2010, and 2010-2019. 

Finally, trend results from the meta-analysis regression method for each season and different time periods were compared "naive" across-year regressions that do not propagate uncertainty to demonstrate how different and potentially misleading conclusions can be obtained.  Trend estimates were compared to 1) trends from ordinary least squares (OLS) regression applied to seasonal averages from observed data and 2) trends from OLS regression applied to GAM seasonal averages. Select examples were used where differences were pronounced to illustrate false positive or negatives that may occur with alternative methods. This analysis was then applied to all stations.

# Results

## Model performance and observed results

```{r, eval = T}
aver2 <- round(100 * mean(modprf$R2), 0)
minr2 <- round(100 * min(modprf$R2), 0)
maxr2 <- round(100 * max(modprf$R2), 0)
minst <- modprf$station[which.min(modprf$R2)]
maxst <- modprf$station[which.max(modprf$R2)]
```

Model predictions for chl-a trends across all stations had an average R-squared value equal to `r aver2`%  (Table \@ref(tab:modprftab)) and ranging from `r minr2`% (station `r minst`) to `r maxr2`% (station `r maxst`). GAM predictions from north to south on the longitudinal axis showed more pronounced annual and seasonal changes in chl-a towards the more southern stations (Figures S1-S9). All the models suggested 1) increasing chl-a from 1990 until 2005 to 2010, followed by decreasing chl-a until the end of the record in 2019, 2) a spring chl-a peak, particularly at southern stations, and 3) a fall chl-a peak that was smaller than the spring peak.  The magnitude of the fall peak did not vary noticeably by location (Figures S1-S9).  

## Trend estimates

To demonstrate the results that can be obtained with the trend tests, estimates of linear trends across years for different seasons are shown for station 34 (Figure \@ref(fig:trnddat)). All plots show trends in seasonal averages within ten-year windows, whereas plots a-c show estimates from January to June and plots d-f show estimates from July to December.  The seasonal trend analyses showed that January to June chl-a increased (log$_{10}$ chl-a slope 0.03 $\mu$g L$^{-1}$ yr$^{-1}$, 0.01-0.06 95\% confidence interval) from 1991 to 2000, whereas a trend for the same period in July to December was not observed.  Chl-a also increased from 2000 to 2010, but only for July to December (log$_{10}$ slope 0.03, 0.01-0.05 95\% confidence interval).  Finally, chl-a decreased from 2010 to 2019 but only for July to December (log$_{10}$ chl-a slope -0.02, -0.04-0 95\% confidence interval).  Because the trends were confined to certain times of the year, the seasonal estimates provide additional information beyond coarser estimates that cover the entire year.  

Temporal changes varied among regions of the Bay and were in fact estimated to be moving in opposite directions.  Figure \@ref(fig:trndmap) shows results from similar analyses as those in figure \@ref(fig:trnddat), but applied to all stations. Mixed-effects meta-analysis regressions applied to seasonal averages showed that increases (based on $p < 0.05$) for the January to June period were observed at stations 32, 34, and 36 from 1991 to 2000 and station 18 from 2000 to 2010, whereas chl-a decreased at stations 30 and 32 from 2010 to 2019. For the July to December period, increases were observed at stations 24, 27, 30, and 32 from 1991 to 2000 and stations 18, 21, 22, and 34 from 2000 to 2010, whereas decreases were observed at stations 30, 32, and 34 from 2010 to 2019.

## Trend comparisons

Results from a ten-year moving window comparison of seasonal trends provided additional context on when significant changes were occurring at each station (Figure \@ref(fig:winchg)).  Trends were observed at all stations that followed a general pattern of increases early in the record followed by decreases later in the record.  Increases and decreases were observed in both the January to June and July to December seasonal periods, with some notable exceptions.  In particular, the most southern stations (32, 34, 36) had increasing trends prior to 2005 only in the July to December period.  Additionally, chl-a at the more northern stations has not changed in recent years for both seasonal periods. For most stations and seasonal periods, a change from increasing to decreasing chl-a occurred around 2007.

Results showing trend estimates from meta-analysis on GAM seasonal estimates provided different conclusions than those from either OLS regression through seasonal averages from observed data (Figure \@ref(fig:trndcmpex) row 1) or OLS regression through GAM estimates (row 2). Figure \@ref(fig:trndcmpex)a shows trend estimates for station 36 for April to June averages from 1991 to 2000.  Only the meta-analysis regression results show a trend in this example (based on $p < 0.05$).  The OLS regression on observed estimates (top plot) and OLS regression on GAM estimates (middle plot) did not identify trends.  Figure \@ref(fig:trndcmpex)b shows trend estimates for station 22 for October to December averages from 2000 to 2010.  Unlike the first example, the top two figures show trends, whereas the bottom plot for the meta-analysis regression does not show a trend.  In both cases, only the meta-analysis results provide accurate trend estimates because of full propagation of uncertainty across methods. 

Applying the same comparison to all stations showed that different trend analysis methods provided conflicting information on the magnitude and significance of the seasonal chl-a changes in each decade (Figure \@ref(fig:trndcmp)).  The slope estimates from the OLS models applied to the observed data were understandably more variable than the slope estimates from the GAM averages and meta-analysis methods, with much larger slopes observed especially at the more southern stations in the January to June period.  Slope estimates from the OLS models applied to the averages from the GAMs as compared to the meta-analysis results were more similar, excluding some of the slope estimates for the southern stations.  Differences in significance of trends between the OLS models applied to the GAM averages and the meta-analyses were also observed, reflecting the ability of the latter to more accurately assess significance of trends by accounting for uncertainty in the average estimates.

# Discussion

The use of the GAM results with second and third stage analyses to assess trends is a new approach that previous trend analysis methods cannot sufficiently address.  Although the development of our approach was motivated by specific questions regarding assessment of seasonal changes over time, the application has advantages over more conventional methods that inadequately account for time series characteristics of water quality data from long-term monitoring programs. In particular, missing observations or irregular sampling can complicate trend assessment and comparison of trends between locations that may differ in sampling design [@Junninen04;@Racault14].  As noted above, non-parametric approaches (i.e., seasonal Kendall tests) are by far the most common trend analysis methods applied to long-term water quality data [@Hirsch82;@Helsel20].  These methods only assess the direction and significance of comparisons between year pairs, and importantly, do not account for full propagation of uncertainty inherent in raw observations.  Aggregation of raw data, e.g., averaging of observations within a year or season to comply with the requirements of Kendall tests, risks loss of information by removing variation between observations at smaller time scales. The logical outcome is increased likelihood of incorrect conclusions from test results.

Our results demonstrated that model structure (i.e., types of smoothers) was less important than allowing the model sufficient freedom to estimate the trends. This is an important conclusion that provides guidance on how GAMs could be used to model time series from long-term water or other environmental monitoring programs when the purpose is to identify and describe which of many possible changes over time have occurred.  Models with separate smoothers for continuous year and day of year can produce nearly identical results in the predicted trends if the knots are sufficiently high to allow the GAMs to be fit as intended by the methods in the mgcv package (Figure \@ref(fig:modsumfig)).  This approach leverages the ability of GAMs to objectively estimate smoothed trends across years by identifying an optimal level of smoothing using generalized cross-validation to extract an underlying signal in the observed data [@Wood04;@Wood17]. Specifying an upper limit for the number of knots that can potentially be used to fit different smoothers is critical to this approach.  A smaller limit can lead to under-smoothing and an insufficient characterization of trends that risks incorrect conclusions from using these models with the second and third stage of analyses. 

Our examples in Figures \@ref(fig:trndcmpex) and \@ref(fig:trndcmp) demonstrate how different conclusions can be obtained if propagation of uncertainty from raw observations across methods is unaccounted for in trend assessment.  These comparisons demonstrate a common approach where raw observations may be aggregated prior to use of more conventional trend analysis methods.  Our assessment of trends using OLS regression applied to seasonal averages from the raw observations is effectively like averaging results within a year and applying a simple Kendall test. In many cases the results may be similar, but loss of information can lead to increased Type I or II error rates depending on characteristics of the raw data.  A false negative result (Type II) was shown in Figure \@ref(fig:trndcmpex)a where a trend was not shown for the OLS regression results, but was observed for the meta-analysis regression results.  Conversely, a false positive result (Type I) was shown in Figure \@ref(fig:trndcmpex)b where the OLS regression results showed a trend, but the meta-analysis results did not.  The potential danger of inflated error rates was further demonstrated at a larger spatial scale across all stations in Figure \@ref(fig:trndcmp).

Variability of sampling effort among years is a common characteristic of environmental monitoring data, which further increases the potential for incorrect conclusions when existing methods have been used. Averages may be skewed in a particular direction if annual estimates are based on a handful of observations from select months [e.g., summer only, @Fouquet12].  The use of GAMs to fit the long-term trend will reduce the potential of limited observations in a particular year skewing estimates of annual or seasonal averages. More importantly, limited observations in a year will be reflected in the standard error estimates derived from the GAM, which has direct implications for how uncertainty is treated in the mixed-effects meta-analyses [@Sera19].  As a result, trend assessments from meta-analyses are more accurate for data with unequal sampling effort where other methods may fail by providing estimates that are biased or have inaccurate or incomplete estimates of uncertainty. 

The underlying cross-validation methods used by GAMs in the mgcv package also reduce the decisions that may be necessary for the implementation of alternative trend assessment methods.  For example, WRTDS and similar smoothing approaches (e.g., LOESS) require decisions on appropriate window widths or bandwidths to define the neighborhood of observations for smoothing [@Hirsch10;@Wan17].  Prior to the implementation of generalized cross-validation in GAMs, these decisions were also required, producing fitted results that are arbitrarily under- or over-smoothed to the raw observations.  This is especially problematic for policy analysis or regulatory decisions if the results change based on arbitrary decisions of the analyst.  Because these decisions are no longer needed for GAMs, the results can be considered a more objective and potentially "true" signal of actual trends that are minimally influenced by process or observation error present in the raw data.  

## Future work

Additional work could be conducted to further strengthen the validity of conclusions based on trends from meta-analysis regression applied to the GAM seasonal averages.  We acknowledge that the third stage analyses require explicit user inputs on year periods to define trends.  Although there are undoubtedly many scenarios where years of interest can be chosen objectively by the needs of an analysis (e.g., regulatory compliance periods, time since management intervention), a more general question of "when" changes occur independent of user decisions is also important to address.  For example, trends could be assessed based on five or ten year moving windows, but which result should be used to inform decisions?  There may be additional methods to develop using objective criteria to more accurately identify inflection points or other important periods where changes occur independent of a user choice.  Assessing water quality changes beyond an evaluation of seasonal averages could also be possible with our approach, such as assessing changes in the timing or magnitude of a seasonal peak across years.

The trend assessment approach does not address causality in the observed trends.  In this regard, the approach is like other trend assessment methods where the focus is on understanding direction, magnitude, and confidence in changes in water quality variables over time. However, a logical follow-up question is "what" is driving the trend after the trend has been adequately described.  This information has obvious implications for management decisions on factors that influence water quality changes, e.g., wastewater treatment upgrades, large-scale climatic factors, or flow regulation practices.  An advantage of GAMs is their flexibility in including alternative predictors, such that the significance of a predictor or comparison of nested models with and without different predictors can provide evidence of which predictors are driving the observed trends [@Wood02;@Zuur09]. In such cases, considerations of model structure can have direct implications on conclusions of potential causality.  As noted above, model structure in describing the long-term trend component was irrelevant for describing long-term trends, although this will depend on the objective of the analysis.  Our goal was to describe chl-a changes relative to time, where the predictors were variations on a general theme (e.g., season vs. year).  This is a different application from using GAMs with predictors selected to assess potential causality.  Therefore, using our approach to assess causality will require considerations of model structure given how GAMs could be used to assess different questions. 

Finally, the evaluation of trends for alternative water quality variables in addition to chl-a is a simple and logical extension of the methods proposed in this study.  The long-term monitoring program maintained by USGS includes multiple parameters in addition to chl-a that can provide additional context into broader water quality trends in the SFE [@Cloern16;@Schraga20]. These parameters include salinity, temperature, light attenuation, dissolved oxygen, suspended particulate matter, and dissolved inorganic nutrients, which collectively can be used to provide a broader understanding of potential eutrophication patterns or ecosystem shifts at seasonal and multi-decadal scales.  Chl-a measurements can also be used to estimate gross primary production to assess process rates that may be more indicative of system function [@Jassby02;@Cloern07].  The open-source wqtrends R package [@Beck21] developed for this manuscript can be used for these analyses to provide additional insight into potential drivers of water quality change in the SFE and other estuarine systems.

# Acknowledgments

We thank the staff of the US Geological Survey that collect and maintain long-term monitoring data in San Francisco Bay.  This work benefited from discussions with the San Francisco Bay Nutrient Technical Workgroup and Steering Committee.  We thank James D. Hagy III for reviewing an earlier draft of this manuscript.

# Figures

```{r, results = 'hide', message = F}
# treatment colors
cls1 <- RColorBrewer::brewer.pal(9, 'Greys')
cls2 <- RColorBrewer::brewer.pal(3, 'BrBG')

toplo1 <- rawdat %>%
  filter(param %in% 'chl') %>% 
  filter(mo %in% c('Aug', 'Sep', 'Oct', 'Nov', 'Dec')) %>% 
  # filter(station %in% c(24, 27, 30, 32)) %>% 
  group_by(yr) %>% 
  mutate(Median = median(value, na.rm = TRUE)) %>% 
  ungroup() 

toplo2 <- rawdat %>%
  filter(param %in% 'chl') %>% 
  mutate(
    yrcat = case_when(
      yr < 2000  ~ '1990-2000', 
      yr >= 2000 & yr < 2010 ~ '2000-2010', 
      yr >= 2010 ~ '2010-2019'
    ), 
    yrcat = factor(yrcat, levels = c('1990-2000', '2000-2010', '2010-2019'))
  )

ylab <- expression(paste("Chl-a (", italic(mu), "g ", L^-1, ")"))

p1 <- ggplot(toplo1, aes(x = yr, y = value, fill = Median, group = yr)) + 
  geom_violin(draw_quantiles = 0.5)  + 
  scale_fill_gradientn(colours = cls1) + 
  scale_y_log10() +
  theme_minimal(base_family = 'serif') +
  theme(
    axis.title.x = element_blank()
  ) +
  labs(
    y = ylab, 
    subtitle = '(a) Annual summer/fall concentrations'
  )

p2 <- ggplot(toplo2, aes(x = mo, y = value, fill = yrcat)) + 
  geom_violin(draw_quantiles = 0.5)  + 
  scale_y_log10(ylab) +
  facet_wrap(~yrcat, ncol = 3) + 
  theme_minimal(base_family = 'serif') +
  scale_fill_manual(values = cls2) +
  theme(
    legend.title = element_blank(), 
    legend.position = 'none', 
    axis.title.x = element_blank(), 
    axis.text.x = element_text(angle = 45, hjust = 1), 
    strip.background = element_blank()
  ) +
  labs(
    y = ylab, 
    subtitle = '(b) Monthly concentrations by decade'
  )

p <- p1 + p2 + plot_layout(ncol = 1)
png('figs/obsdat.png', height = 7, width = 8, family = 'serif', units = 'in', res = 400)
print(p)
dev.off()
```
```{r obsdat, fig.cap = "Observed chl-a concentrations for all stations in central and south San Francisco Estuary (18-36, Figure \\@ref(fig:sitemap)), with (a) annual summer/fall concentrations (Aug - Dec) and (b) monthly concentrations by decade."}
knitr::include_graphics('figs/obsdat.png')
```

```{r, results = 'hide', message = F}

# station locations
sflocs <- locs %>% 
  st_as_sf(coords = c('lon', 'lat'), crs = 4326)

# for inset
states <- us_states() %>% 
  filter(!name %in% c('Alaska', 'Hawaii', 'Puerto Rico'))
castate <- states %>% 
  filter(name %in% 'California')
continent <- getMap(resolution = 'low') %>% 
  st_as_sf %>% 
  filter(continent %in% c('North America', 'South America')) %>% 
  st_transform(crs = st_crs(states)) %>% 
  st_buffer(dist = 0) %>% 
  st_union()
locbuff <- st_buffer(sflocs, dist = 0.1)
locbuff2 <- st_buffer(sflocs, dist = 0.07)
insetbb <- st_buffer(sflocs, dist = 1.5) %>% 
  st_bbox() %>% 
  st_as_sfc(crs = 4326)
statebuff <- st_buffer(states, dist = 10)
insetylim <- st_bbox(statebuff)[c('ymin', 'ymax')]
insetxlim <- st_bbox(statebuff)[c('xmin', 'xmax')]

# basemap
dat_ext <- unname(st_bbox(locbuff))
bsmap1 <- get_stamenmap(bbox = dat_ext, maptype = 'terrain-background', zoom = 11)

# change opacity of basemap
mapatt <- attributes(bsmap1)
bsmap1_transparent <- matrix(adjustcolor(bsmap1, 
                                         alpha.f = 0.75), 
                             nrow = nrow(bsmap1))
attributes(bsmap1_transparent) <- mapatt

# basemape plus stations
p1 <- ggmap(bsmap1_transparent) +
  geom_sf(data = sflocs, inherit.aes = F, col = 'black', pch = 21, fill = 'grey', size = 4) + 
  geom_text(data = locs, aes(x = lon + 0.02, y = lat + 0.02, label = station), fontface = 'bold.italic') + 
  theme(
    axis.title = element_blank()
  ) + 
  north(locbuff, scale = 0.15, symbol = 10, location = 'bottomright') +
  scalebar(locbuff2, dist = 10, dist_unit = 'km', location = 'bottomleft', transform = T, 
           st.color = 'black', border.size = 0.5, st.dist = 0.02, st.size = 4) 

p2 <- ggplot() + 
  geom_sf(data = continent, fill = 'grey', colour = 'grey') + 
  # geom_sf(data = states, fill = 'grey', colour = 'grey') + 
  geom_sf(data = insetbb, fill = NA, color = 'blue', size = 1.5) +
  coord_sf(ylim = insetylim, xlim = insetxlim) + 
  theme_void() +
  theme( 
    panel.background = element_rect(fill = '#FFFFFF99', colour = 'white'), 
    panel.border = element_rect(colour = 'black', fill = 'transparent')
  )

# final map
p <- p1 + 
  inset(ggplotGrob(p2), xmin = -122.2, xmax = -121.97, ymin = 37.75, ymax = 37.983)

# save as png
png('figs/sitemap.png', height = 6.5, width = 5, units = 'in', family = 'serif', res = 300)
print(p)
dev.off()
```
```{r sitemap, fig.cap = 'Station locations in the central and south San Francisco Estuary used for analysis.  See Table \\@ref(tab:sumtab) for station descriptions.  Full dataset described in @Schraga20.'}
knitr::include_graphics('figs/sitemap.png')
```

```{r, results = 'hide'}
# use previously fitted list of models
mods <- modstr[2:4]

thm <-theme_minimal() + 
  theme(
    legend.title = element_blank(),
    legend.position = 'right'
  )
lncol <- 'tomato1'
alph <- 0.5
ylm <- c(0.8, 100)

p1a <- show_prddoy(mods[[1]], ylab = 'SY: Chl-a (ug/L)', size = 0.8, alpha = 0.8) + 
  guides(colour = ggplot2::guide_colourbar(barheight = 15, barwidth = 1)) +
  scale_colour_gradient(low = "white", high = "black") + 
  coord_cartesian(ylim = ylm) +
  theme_minimal() + 
  theme(
    legend.position = 'none', 
    axis.title.x = element_blank()
  ) +
  labs(
    title = '(a) Model predictions by day of year'
  )
p1b <- show_prddoy(modstr[[2]], ylab = 'SYD: Chl-a (ug/L)', size = 0.8, alpha = 0.8) + 
  guides(colour = ggplot2::guide_colourbar(barheight = 15, barwidth = 1)) +
  scale_colour_gradient(low = "white", high = "black") + 
  coord_cartesian(ylim = ylm) +
  theme_minimal() + 
  theme(
    legend.position = 'none'
  )
p1c <- show_prddoy(mods[[3]], ylab = 'SYDI: Chl-a (ug/L)', size = 0.8, alpha = 0.8) + 
  guides(colour = ggplot2::guide_colourbar(barheight = 8, barwidth = 1)) +
  scale_colour_gradient('Year', low = "white", high = "black") + 
  coord_cartesian(ylim = ylm) +
  theme_minimal() + 
  theme(
    legend.position = 'right', 
    axis.title.x = element_blank()
  )

p1 <- p1a + p1b + p1c + plot_layout(ncol = 3, widths = c(1, 1, 1))
prds <- lapply(modstr, anlz_prd) %>% 
  enframe('model', 'dat') %>%
  unnest('dat')

toplo2 <- prds %>% 
  select(model, date, value) %>% 
  spread(model, value)

toplo3 <- prds %>% 
  select(model, date, annvalue) %>% 
  spread(model, annvalue)

p2a <- ggplot(toplo2, aes(x = SY, y = SYD)) + 
  geom_point(alpha = alph) + 
  geom_abline(slope = 1, intercept = 0, color = lncol) + 
  scale_y_log10(limits = ylm) + 
  scale_x_log10(limits = ylm) + 
  thm + 
  labs(
    title = '(b) Estimated chl-a between models'
  )
p2b <- ggplot(toplo2, aes(x = SYD, y = SYDI)) + 
  geom_point(alpha = alph) + 
  geom_abline(slope = 1, intercept = 0, color = lncol) + 
  scale_y_log10(limits = ylm) + 
  scale_x_log10(limits = ylm) + 
  thm
p2c <- ggplot(toplo2, aes(x = SY, y = SYDI)) + 
  geom_point(alpha = alph) + 
  geom_abline(slope = 1, intercept = 0, color = lncol) + 
  scale_y_log10(limits = ylm) + 
  scale_x_log10(limits = ylm) + 
  thm

p2 <- p2a + p2b + p2c + plot_layout(ncol = 3)


p3a <- ggplot(toplo3, aes(x = SY, y = SYD)) + 
  geom_point(alpha = alph) + 
  geom_abline(slope = 1, intercept = 0, color = lncol) + 
  scale_y_log10(limits = ylm) + 
  scale_x_log10(limits = ylm) + 
  thm +
  labs(
    title = '(c) Estimated smoother for continuous year between models'
    )
p3b <- ggplot(toplo3, aes(x = SYD, y = SYDI)) + 
  geom_point(alpha = alph) + 
  geom_abline(slope = 1, intercept = 0, color = lncol) + 
  scale_y_log10(limits = ylm) + 
  scale_x_log10(limits = ylm) + 
  thm
p3c <- ggplot(toplo3, aes(x = SY, y = SYDI)) + 
  geom_point(alpha = alph) + 
  geom_abline(slope = 1, intercept = 0, color = lncol) + 
  scale_y_log10(limits = ylm) + 
  scale_x_log10(limits = ylm) + 
  thm

p3 <- p3a + p3b + p3c + plot_layout(ncol = 3)

out <- p1 / p2 / p3
png('figs/modsumfig.png', height = 7, width = 8, units = 'in', family = 'serif', res = 400)
print(out)
dev.off()
```
```{r modsumfig, fig.cap = "GAM output of estimated chl-a for models SY, SYD, and SYDI.  Model S is identical to SY and is not shown.  Plots in (a) show model predictions by day of year with separate lines for each year.  Plots in (b) show pairwise comparisons of predicted chl-a between the models and plots in (c) show the same comparisons as in (b) but only for results from the estimated smoother for the `cont_year` variable.  The plots demonstrate that results between the models are comparable except for a few observations at extreme values (a, b), but they vary in the penalties applied to the basis functions for any particular smoother depending on which additive components are included in each model (c).  The 1:1 lines are in red to facilitate comparisons."}
knitr::include_graphics('figs/modsumfig.png')
```

```{r, results = 'hide', message = F}
mod <- modslog_chl34 %>%
  pull(model) %>% 
  deframe()

seas <- tibble(
  doystr = c(41, 41, 41, 213, 213, 213), 
  doyend = c(213, 213, 213, 338, 338, 338), 
  yrstr = c(1991, 2000, 2010, 1991, 2000, 2010), 
  yrend = c(2000, 2010, 2019, 2000, 2010, 2019)
  ) %>% 
  mutate(
    est = purrr::pmap(list(doystr, doyend, yrstr, yrend), function(doystr, doyend, yrstr, yrend){
      
      mixmet <- anlz_avgseason(mod, doystr = doystr, doyend = doyend) %>% 
        anlz_mixmeta(yrstr = yrstr, yrend = yrend)

      slope <- summary(mixmet)$coefficients[2, c(1, 5, 6)]
      slope <- round(slope, 2)
      slope <- paste0(slope[1], ' (', slope[2], ', ', slope[3], ')')
      
      pval <- mixmet %>% summary %>% coefficients %>% .[2, 4]
      
      out <- data.frame(est = slope, pval = pval)
     
      return(out)
      
    })
  ) %>% 
  unnest(est) %>% 
  mutate(
    lets = c('(a)', '(b)', '(c)', '(d)', '(e)', '(f)'), 
    doystr = ifelse(doystr == 41, 'Jan', 'Jul'), 
    doyend = ifelse(doyend == 213, 'Jun', 'Dec'), 
    pval = p_ast(pval)
  ) %>% 
  unite('seas', doystr, doyend, sep = '-') %>% 
  unite('yrs', yrstr, yrend, sep = '-') %>% 
  unite('lets', lets, yrs, sep = ' ') %>% 
  unite('lets', lets, seas, est, pval, sep = ', ') %>% 
  deframe

ylim <- c(0, 21)
ylab <- expression(paste("Chl-a (", mu, "g ", L^-1, ")"))

p1 <- show_avgseason(mod, doystr = 41, doyend = 213, yrstr = 1991, yrend = 2000, ylab = ylab) +
  coord_cartesian(ylim = ylim) + 
  labs(
    subtitle = seas[[1]],
    title = NULL
  )
p2 <- show_avgseason(mod, doystr = 41, doyend = 213, yrstr = 2000, yrend = 2010, ylab = NULL) +
  coord_cartesian(ylim = ylim) + 
  labs(
    subtitle = seas[[2]], 
    title = NULL
  )
p3 <- show_avgseason(mod, doystr = 41, doyend = 213, yrstr = 2010, yrend = 2019, ylab = NULL) +
  coord_cartesian(ylim = ylim) +  
  labs(
    subtitle = seas[[3]], 
    title = NULL
  )
p4 <- show_avgseason(mod, doystr = 213, doyend = 338, yrstr = 1991, yrend = 2000, ylab = ylab) +
  coord_cartesian(ylim = ylim) + 
  labs(
    subtitle = seas[[4]], 
    title = NULL
  )
p5 <- show_avgseason(mod, doystr = 213, doyend = 338, yrstr = 2000, yrend = 2010, ylab = NULL) +
  coord_cartesian(ylim = ylim) + 
  labs(
    subtitle = seas[[5]], 
    title = NULL
  )
p6 <- show_avgseason(mod, doystr = 213, doyend = 338, yrstr = 2010, yrend = 2019, ylab = NULL) +
  coord_cartesian(ylim = ylim) + 
  labs(
    subtitle = seas[[6]],
    title = NULL
  )

p <- p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(nrow = 2) & 
  theme_minimal(base_family = 'serif', base_size = 12) &
  theme(
    axis.title.x = element_blank(),
    legend.position = 'none'
    )

png('figs/trnddat.png', height = 6, width = 11, family = 'serif', units = 'in', res = 400)
print(p)
dev.off()
```
```{r trnddat, fig.cap = "Examples of seasonal averages and trend estimates in ten year blocks from meta-analyses using results of GAM predictions for station 34.  Plots (a), (b), and (c) show trend estimates for January through June and (d), (e), and (f) show trend estimates for July through December.  The trend lines estimate the rate of change of chl-a per year, reported as the log$_{10}$-slope (+/- 95 % confidence interval) in the plot title. ns: not significant at $\\alpha$ = 0.05, * p < 0.05, ** p < 0.005"}
knitr::include_graphics('figs/trnddat.png')
```

```{r, results = 'hide', message = F}
# change opacity of basemap
mapatt <- attributes(bsmap1)
bsmap1_transparent <- matrix(adjustcolor(bsmap1, 
                                         alpha.f = 0.4), 
                             nrow = nrow(bsmap1))
attributes(bsmap1_transparent) <- mapatt

leglab <- expression(paste(log[10], " chl-a change (", italic(mu), "g ", L^-1, yr^-1, ")"))

pthm <- theme_bw(base_family = 'serif', base_size = 12) +
  theme(
    legend.position = 'top',
    legend.box = 'vertical', 
    strip.background = element_blank(),
    axis.title = element_blank(), 
    axis.text = element_text(size = 8)
  )

toplo <- seastrnd %>%
  mutate(
    station = as.numeric(station),
    pval = ifelse(pval < 0.05, '*', ''), 
    coefsgn = sign(yrcoef), 
    coefsgn = factor(coefsgn, levels = c('1', '-1'), labels = c('inc', 'dec'))
  ) %>% 
  left_join(locs, by = 'station')

p <- ggmap(bsmap1_transparent) +
  geom_point(data = toplo, aes(x = lon, y = lat, size = abs(yrcoef), shape = coefsgn, fill = yrcoef, colour = pval)) + #, color = 'black') +
  geom_text(data = toplo, aes(x = lon, y = lat, label = pval), nudge_x = 0.04) + 
  facet_grid(seas ~ yrs) + 
  scale_fill_gradient2(leglab, low = 'green', mid = 'white',  high = 'tomato1', midpoint = 0) +
  scale_colour_manual(values = c('black', 'black'), guide = F) +
  coord_map() + 
  scale_shape_manual('Trend', values = c(24, 25)) + 
  pthm +
  scale_size(range = c(1, 6), guide = F) +
  guides(fill = guide_colourbar(barheight = 0.5, barwidth = 8)) 

png('figs/trndmap.png', height = 6.5, width = 7.5, family = 'serif', units = 'in', res = 400)
p
dev.off()
```
```{r trndmap, fig.cap= "Trend estimates across seasons by decade for chl-a at each station. Point type and color represent the direction and magnitude of an estimated trend as the log$_{10}$ slope for chl-a concentration per year.  Trends with $p<0.05$ are marked with an asterisk.  All results are from Model `S`."}
knitr::include_graphics('figs/trndmap.png')
```

```{r, results = 'hide', message = F}

toplo <- seastrnd2 %>% 
  na.omit() %>% 
  mutate(
    subttl = ifelse(doystr < 150, '(a) Jan-Jun', '(b) Jul-Dec'),
    pval = dplyr::case_when(
      pval < 0.05 ~ 'p < 0.05', 
      T ~ 'ns'
    ), 
    pval = factor(pval, levels = c('ns', 'p < 0.05'))
  ) %>% 
  dplyr::filter(yr <= 2014 & yr >= 1996)

p <- ggplot(data = toplo, aes(x = yr, y = yrcoef, fill = pval)) + 
  geom_errorbar(ggplot2::aes(ymin = yrcoef_lwr, ymax = yrcoef_upr, color = pval), width = 0) +
  geom_point(shape = 21, size = 3) +
  scale_x_continuous(limits = c(1996, 2014), breaks = seq(1996, 2014)) +
  # scale_y_continuous(limits = c(-2, 2)) +
  scale_color_manual(values = c('black', 'tomato1'), drop = FALSE) +
  scale_fill_manual(values = c('white', 'tomato1'), drop = FALSE) +
  geom_hline(yintercept = 0) + 
  facet_grid(station ~ subttl) + 
  theme_minimal(base_family = 'serif') + 
  theme(
    axis.title.x = element_blank(), 
    legend.position = 'right', 
    legend.title = element_blank(),
    strip.background = element_blank(), 
    strip.text = element_text(size = 13),
    axis.text.x = element_text(size = 8, angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    # panel.grid.major.y = element_blank(), 
    strip.text.x = element_text(size = 12, hjust = 0), 
    axis.ticks = element_line(), 
    panel.spacing.y = grid::unit(1, "lines")
  ) +
  labs(
    y = expression(paste(log[10], ' chl-a change ( ', mu, 'g ', L^-1, yr^-1, ')'))
  )

png('figs/winchg.png', height = 8, width = 8, family = 'serif', units = 'in', res = 400)
print(p)
dev.off()
```
```{r winchg, fig.cap='Estimates of log$_{10}$ chl-a change per year (+/- 95\\% confidence interval) from applying the meta-analysis across the seasonal averages for each station.  Stations are arranged top to bottom from north to south.  Plots in (a) show estimates for seasonal averages from January to June and plots in (b) show estimates for seasonal averages from July to December. Results are from a ten-year, centered moving window where each point shows a linear trend estimate from five years prior to after each year. Estimates prior to 1996 and after 2014 are not available because of an incomplete ten year record for estimating the trend. Estimates in a year that are significant are shown in red.'}
knitr::include_graphics('figs/winchg.png')
```

```{r, results = 'hide', message = F}
show_cmpplo <- function(fl, doystr, doyend, yrstr, yrend, ttl, ymax = NULL, 
                        ylab = expression(paste("Chl-a (", italic(mu), "g ", L^-1, ")"))){
    
  load(file = fl)
  nm <- basename(fl)
  nm <- gsub('\\.RData', '', nm)
  mod <- get(nm) %>%
    pull(model) %>%
    deframe()
  
  # data to model
  tomod1 <- mod$model %>%
    mutate(
      yr = floor(cont_year),
      doy = yday(date_decimal(cont_year))
    ) %>%
    filter(doy >= doystr & doy <= doyend) %>%
    group_by(yr) %>%
    summarise(
      avg = mean(value, na.rm = T),
      sdavg = sd(value, na.rm = T),
      .groups = 'drop'
      ) %>%
    mutate(bt_avg = 10^(avg + (sdavg^2) / 2))
  avgseas <- anlz_avgseason(mod = mod, doystr = doystr, doyend = doyend)
  tomod2 <- avgseas %>%
    filter(yr <= yrend & yr >= yrstr) %>%
    mutate(S = se^2)
  
  # model
  obmod <- tomod1 %>%
    filter(yr >= yrstr & yr <= yrend) %>%
    lm(avg ~ yr, .)
  lmmod <- lm(avg ~ yr, tomod2)
  mxmod <- mixmeta::mixmeta(avg ~ yr, S = S, random = ~1|yr, data = tomod2, method = 'reml')
  
  # pvalues
  pval1 <- summary(obmod) %>% coefficients %>% .[2, 4]
  pval2 <- summary(lmmod) %>% coefficients %>% .[2, 4]
  pval3 <- summary(mxmod) %>% coefficients %>% .[2, 4]
  pval1 <- paste0('p = ', round(pval1, 2), ', ', p_ast(pval1))
  pval2 <- paste0('p = ', round(pval2, 2), ', ', p_ast(pval2))
  pval3 <- paste0('p = ', round(pval3, 2), ', ', p_ast(pval3))
  
  dispersion <- summary(mod)$dispersion
  s2a <- (summary(obmod)$sigma)^2
  s2b <- (summary(lmmod)$sigma)^2
  
  toplo1 <- data.frame(
      yr = seq(yrstr, yrend, length = 50)
    ) %>%
    dplyr::mutate(
      avg = predict(obmod, newdata = data.frame(yr = yr)),
      se = predict(obmod, newdata = data.frame(yr = yr), se.fit = T)$se.fit,
      bt_lwr = 10^((avg - 1.96 * se) + log(10) * s2a / 2),
      bt_upr = 10^((avg + 1.96 * se) + log(10) * s2a / 2),
      bt_avg = 10^(avg + log(10) * s2a / 2)
    )
  
  toplo2 <- data.frame(
    yr = seq(yrstr, yrend, length = 50)
    ) %>%
    dplyr::mutate(
      avg = predict(lmmod, newdata = data.frame(yr = yr)),
      se = predict(lmmod, newdata = data.frame(yr = yr), se.fit = T)$se.fit,
      bt_lwr = 10^((avg - 1.96 * se) + log(10) * dispersion / 2),
      bt_upr = 10^((avg + 1.96 * se) + log(10) * dispersion / 2),
      bt_avg = 10^(avg + log(10) * dispersion / 2)
    )
  
  toplo3 <- data.frame(
    yr = seq(yrstr, yrend, length = 50)
  ) %>%
    dplyr::mutate(
      avg = predict(mxmod, newdata = data.frame(yr = yr)),
      se = predict(mxmod, newdata = data.frame(yr = yr), se = T)[, 2],
      bt_lwr = 10^((avg - 1.96 * se) + log(10) * dispersion / 2),
      bt_upr = 10^((avg + 1.96 * se) + log(10) * dispersion / 2),
      bt_avg = 10^(avg + log(10) * dispersion / 2)
    )
  
  # y axis limits
  ylim <- c(0, ymax)
  if(is.null(ymax))
    ylim <- c(0, max(avgseas$bt_upr))
  
  # plots
  p1 <- ggplot(data = tomod1, aes(x = yr, y = bt_avg)) +
    geom_point(colour = 'deepskyblue3') +
    geom_ribbon(data = toplo1, aes(ymin = bt_lwr, ymax = bt_upr), fill = 'grey', alpha = 0.4) +
    geom_line(data = toplo1, color = 'grey') +
    theme_minimal(base_family = 'serif') +
    coord_cartesian(ylim = ylim, xlim = c(1990, 2020)) +
    theme(
      axis.title.x = element_blank(),
      axis.text.x = element_blank()
    ) +
    labs(
      title = ttl,
      subtitle = paste('OLS regression on raw data,', pval1),
      y = NULL
    )
  
  p2 <- ggplot(data = avgseas, aes(x = yr, y = bt_avg)) +
    geom_point(colour = 'deepskyblue3') +
    geom_ribbon(data = toplo2, aes(ymin = bt_lwr, ymax = bt_upr), fill = 'lightgreen', alpha = 0.4) +
    geom_line(data = toplo2, color = 'green') +
    theme_minimal(base_family = 'serif') +
    coord_cartesian(ylim = ylim, xlim = c(1990, 2020)) +
    theme(
      axis.title.x = element_blank(),
      axis.text.x = element_blank()
    ) +
    labs(
      subtitle = paste('OLS regression on GAM estimates,', pval2),
      y = ylab
    )
  
  p3 <- ggplot(data = avgseas, aes(x = yr, y = bt_avg)) +
    geom_point(colour = 'deepskyblue3') +
    geom_errorbar(aes(ymin = bt_lwr, ymax = bt_upr), colour = 'deepskyblue3') +
    geom_ribbon(data = toplo3, aes(ymin = bt_lwr, ymax = bt_upr), fill = 'pink', alpha = 0.4) +
    geom_line(data = toplo3, color = 'pink') +
    theme_minimal(base_family = 'serif') +
    coord_cartesian(ylim = ylim, xlim = c(1990, 2020)) +
    theme(
      axis.title.x = element_blank()
    ) +
    labs(
      subtitle = paste('Meta-analysis regression on GAM estimates', pval3),
      y = NULL
    )
  
  p <- p1 + p2 + p3 + plot_layout(ncol = 1)
  
  return(p)

}

p1 <- show_cmpplo('data/modslog_chl36.RData', doystr = 91, doyend = 181, yrstr = 1991, yrend = 2000, 
                  ttl =  '(a) Station 36, Apr-Jun chl. averages', ymax = NULL)
p2 <- show_cmpplo('data/modslog_chl22.RData', doystr = 274, doyend = 364, yrstr = 2000, yrend = 2010, 
                  ttl =  '(b) Station 22, Oct-Dec chl. averages', ymax = NULL, ylab = NULL)
# p3 <- show_cmpplo('data/modslog_chl34.RData', doystr = 274, doyend = 364, yrstr = 2010, yrend = 2019, 
#                   ttl =  '(c) Station 34, Oct-Dec chl. averages', ymax = 15, ylab = NULL)

p <- (p1 |  p2) + plot_layout(ncol = 2)
png('figs/trndcmpex.png', height = 6.5, width = 7.75, family = 'serif', units = 'in', res = 400)
p
dev.off()
```
```{r trndcmpex, fig.cap = 'Trend estimate comparisons for three models applied to seasonal averages of chl-a in different annual periods at  (a) station 36 and (b) station 22. The first row shows OLS (ordinary least squares) regression applied to annual averages of chl-a from the observed data, the second row shows OLS regression applied to annual averages of chl-a from the GAM, and the third row shows meta-analysis regression applied to the annual averages of chl-a from the GAM.  Regressions in each plot are fit through the seasonal estimates indicated in the plot titles for a specified year range.'}
knitr::include_graphics('figs/trndcmpex.png')
```

```{r, results = 'hide', message = F}
toplo <- cmptrnd
p <- ggplot(toplo, aes(y = station, x = yrcoef, shape = modtyp, colour = pval)) + 
  facet_grid(seas ~ yrs, scales = 'free_x') + 
  geom_point(size = 3, position = position_dodge(width = 0.5), alpha = 0.6) + 
  scale_shape('Trend model') + 
  scale_colour_discrete_diverging(name = 'Trend', palette = 'BlueRed2') +
  geom_vline(xintercept = 0, linetype = 'dashed') + 
  # scale_x_continuous(limits = c(-1.14, 1.14)) +
  theme_minimal() + 
  theme(
    strip.background = element_blank(),
    panel.background = element_rect(fill = 'gray98', colour = NA), 
    panel.border = element_blank(), 
    legend.position = 'top'
    ) +
  labs(
    y = 'Station (north to south)', 
    x = expression(paste(log[10], " chl-a change (", italic(mu), "g ", L^-1, yr^-1, ")"))
  )

png('figs/trndcmp.png', height = 4.5, width = 7.5, family = 'serif', units = 'in', res = 400)
p
dev.off()
```
```{r trndcmp, fig.cap = 'Trend estimate comparisons for three models applied to seasonal averages of chl-a in different annual periods at each station. The "OLS observed" trend model is based on an ordinary least squares (OLS) regression fit to the annual averages of chl-a from the observed data, the "OLS GAM" trend model is based on an OLS regression fit to the annual averages of chl-a from the GAM model, and the "Meta-analysis GAM" trend model is based on a meta-analysis regression fit to the annual averages of chl-a from the GAM model.  Values for each model are the log$_{10}$-slope estimates as annual change per year within each season, with color denoting trend significance.'}
knitr::include_graphics('figs/trndcmp.png')
```

# Tables

```{r sumtab, tab.cap = 'Station locations, sample sizes, and summary values (median, minimum, maximum) for chl-a ($\\mu$g L$^{-1}$) .  Rows are arranged from north to south.'}
totab <- rawdat %>% 
  filter(param %in% c('chl')) %>% 
  group_by(station, param) %>% 
  summarise(
    n = n(), 
    medv = median(value, na.rm = T), 
    minv = min(value, na.rm = T), 
    maxv = max(value, na.rm = T)
  ) %>% 
  ungroup() %>% 
  gather('var', 'val', n, medv, minv, maxv) %>% 
  unite('var', param, var) %>% 
  mutate(var = factor(var, levels = c('chl_n', 'chl_medv', 'chl_minv', 'chl_maxv'))) %>% 
  left_join(locs, by = 'station') %>% 
  spread(var, val) %>% 
  mutate_at(c('chl_medv', 'chl_minv', 'chl_maxv'), round, 1) %>% 
  mutate_at(c('lat', 'lon'), round, 3)

tab <- flextable(totab) %>% 
  set_header_labels(
    station = 'Station', 
    lat = 'Latitude', 
    lon = 'Longitude', 
    chl_n = 'n', 
    chl_medv = 'Med.', 
    chl_minv = 'Min.', 
    chl_maxv = 'Max.'
  ) %>% 
  # add_header_row(values = c('', '', '', 'Chl-a (ug/L)', '', '', '')) %>% 
  # merge_at(i = 1, j = 4:7, part = 'header') %>% 
  # colformat_num(j = c(5, 6, 7), digits = 1) %>%
  # colformat_num(j = c(2, 3), digits = 3) %>% 
  align(align = 'right', part = 'header') %>% 
  border_remove() %>% 
  hline_top(border = fp_border(color = 'black')) %>% 
  font(fontname = 'Times', part = 'all') %>%
  fontsize(size = 12, part = 'all') %>% 
  width(j = c(2, 3), width = 0.9) 
tab
```

```{r modsumtab, eval = T}
tab <- tibble(
  GAM = c('`S`', '`SY`', '`SYD`', '`SYDI`'),
  `Additive components` = c(
    '`s(cont_year)`', 
    '`cont_year + s(cont_year)`',
    '`cont_year + s(cont_year) + s(doy)`', 
    '`cont_year + s(cont_year) + s(doy) + ti(cont_year, doy)`'
  ),
  Details = c(
    'A single smoother over a continuous year variable', 
    'A linear continuous year variable and a single smoother over a continuous year variable', 
    'A linear continuous year variable, a smoother over a continuous year variable, and smoother over a day of year variable',
    'A linear continuous year variable, a smoother over a continuous year variable, smoother over a day of year variable, and an interaction smoother across continuous year and day of year variables')
  )

# table stuff
cap.val <- 'Summary and details for each of the GAM structures.  In practice, a sufficiently large number of knots provided to the additive terms will produce identical or comparable estimates for a response variable.  The models differ in the allocation of penalties for the smoothness of each spline (`s()`).'

# table
knitr::kable(tab, booktabs = T, caption = cap.val)
```

```{r modstrtab}
totab <- modstr %>% 
  enframe() %>% 
  mutate(value = purrr::map(value, function(x){
    
    GCV <- as.numeric(x$gcv.ubre)
    R2 <- summary(x)$r.sq 
    smooths <- summary(x)$s.table %>% data.frame %>% tibble::rownames_to_column('smoother')
    
    out <- data.frame(GCV = GCV, R2 = R2, smooths)
    
    return(out)
    
  })) %>% 
  unnest('value') %>% 
  select(-Ref.df) %>% 
  mutate(p.value = p_ast(p.value)) %>% 
  mutate_if(is.numeric, round, 2) %>% 
  mutate(
    GCV = ifelse(duplicated(name), '', GCV),
    R2 = ifelse(duplicated(name), '', R2),
    name = ifelse(duplicated(name), '', name)
  ) %>% 
  rename(
    model = name, 
    `p-val` = p.value
  )

# table stuff
cap.val <- 'Comparison of the four model structures (S, SY, SYD, SYDI) described in the first stage analysis of GAM estimation.  The four models provide either identical or comparable ability to describe chl-a trends at an example station (32) in the southern end of the San Francisco Estuary.  The models differ in additive smoothers and the amount of effective degrees of freedom (edf) in the smoothers (measure of wiggliness in each component), but the overall model predictions are comparable. GCV: generalized cross-validation score, R2: r-squared values for predictions, edf: effective degrees of freedom, F: F-statistic, p-val: probability value, ** p < 0.001'

# table
knitr::kable(totab, booktabs = T, caption = cap.val)
```

```{r modprftab}
totab <- modprf %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(`R-squared` = R2)

cap <- 'Model performance statistics for each station as generalized cross-validation scores (GCV) and r-squared values.'

knitr::kable(totab, booktabs = T, caption = cap)
```

# References
